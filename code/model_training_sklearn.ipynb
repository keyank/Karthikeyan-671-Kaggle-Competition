{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Model Exploration\n",
    "\n",
    "\n",
    "# Random Forest \n",
    "\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "    1. Restart and run all should work (change the paths to input and output properly)\n",
    "    2. The code is written in such a way that if the program stops in between, it will continue from there not from the begining (atleast for Random Forest and XGBoost)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import  linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB, CategoricalNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/coupons/train.csv'\n",
    "test_path = '../data/coupons/test.csv'\n",
    "sample_path = '../data/coupons/sample_submission1.csv'\n",
    "\n",
    "data = pd.read_csv(train_path)\n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_first = False\n",
    "prune=False\n",
    "\n",
    "X_df, y_df = clean_all(data, drop_first, prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_first = False\n",
    "prune=False\n",
    "\n",
    "X_test_df, _ = clean_all(test_data, drop_first, prune)\n",
    "\n",
    "X_test = np.array(X_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(n_est, max_depth , gamma)\n",
    "clf = xgb.XGBClassifier(n_estimators=n_est, max_depth=max_depth, gamma=gamma)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "hyper_pram_list = [\n",
    "    (100, 10, 0.8), (100, 10, 0.9), (100, 10, 1), \n",
    "    (100, 15, 0.8), (100, 15, 0.9), (100, 15, 1), \n",
    "    (100, 20, 0.8), (100, 20, 0.9), (100, 20, 1),\n",
    "    (200, 5, 0.8), (200, 5, 0.9), (200, 5, 1), \n",
    "    (200, 8, 0.8), (200, 8, 0.9), (200, 8, 1), \n",
    "    (200, 10, 0.8), (200, 10, 0.9), (200, 10, 1), \n",
    "    (200, 15, 0.8), (200, 15, 0.9), (200, 15, 1), \n",
    "    (200, 20, 0.8), (200, 20, 0.9), (200, 20, 1),\n",
    "    (500, 5, 0.8), (500, 5, 0.9), (500, 5, 1), \n",
    "    (500, 8, 0.8), (500, 8, 0.9), (500, 8, 1), \n",
    "    (500, 10, 0.8), (500, 10, 0.9), (500, 10, 1), \n",
    "    (500, 15, 0.8), (500, 15, 0.9), (500, 15, 1), \n",
    "    (500, 20, 0.8),(500, 20, 0.9), (500, 20, 1),\n",
    "]\n",
    "\n",
    "print(len(hyper_pram_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "\n",
    "try: \n",
    "    filename = 'xgboost_predictions_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    all_y_preds = pickle.load(open(filepath, 'rb+') )\n",
    "    \n",
    "    filename = 'xgboost_accs_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    all_eval_acc = pickle.load(open(filepath, 'rb+') )\n",
    "except: \n",
    "    all_y_preds = []\n",
    "    all_eval_acc = []\n",
    "\n",
    "\n",
    "\n",
    "for i, (n_est, max_depth , gamma) in enumerate(hyper_pram_list): \n",
    "    \n",
    "    if i < len(all_y_preds): \n",
    "        continue\n",
    "    \n",
    "    avg_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        clf = xgb.XGBClassifier(n_estimators=n_est, max_depth=max_depth, gamma=gamma)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_val, y_val)\n",
    "        avg_score += score \n",
    "\n",
    "    avg_score = avg_score/n_splits\n",
    "    \n",
    "    print( (n_est, max_depth , gamma), '\\t AVG:', avg_score,  '\\n')\n",
    "\n",
    "\n",
    "    clf = xgb.XGBClassifier(n_estimators=n_est, max_depth=max_depth, gamma=gamma)\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "    all_y_preds.append(y_pred)\n",
    "    all_eval_acc.append(avg_score)\n",
    "    \n",
    "    \n",
    "    filename = 'xgboost_predictions_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    pickle.dump(all_y_preds, open(filepath, 'wb+') )\n",
    "\n",
    "\n",
    "    filename = 'xgboost_accs_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    pickle.dump(all_eval_acc, open(filepath, 'wb+') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_pram_list = [\n",
    "    (500, 'entropy', 'auto', True),  (500, 'entropy', 'auto', True),  \n",
    "    (500, 'entropy', 0.05, True), (500, 'entropy', 0.1, True), (500, 'entropy', 0.2, True), \n",
    "    \n",
    "    (200, 'entropy', 'auto', True), (200, 'entropy', 0.05, True), \n",
    "    (200, 'entropy', 0.1, True), (200, 'entropy', 0.2, True), \n",
    "    \n",
    "    (300, 'entropy', 'auto', True), (300, 'entropy', 0.05, True),  \n",
    "    (300, 'entropy', 0.1, True), (300, 'entropy', 0.2, True), \n",
    "    \n",
    "    (500, 'gini', 'auto', True), (500, 'gini', 0.05, True), \n",
    "    (500, 'gini', 0.1, True), (500, 'gini', 0.2, True), \n",
    "    \n",
    "    (200, 'gini', 'auto', True), (200, 'gini', 0.05, True), \n",
    "    (200, 'gini', 0.1, True), (200, 'gini', 0.2, True), \n",
    "    \n",
    "    (300, 'gini', 'auto', True), (300, 'gini', 0.05, True), \n",
    "    (300, 'gini', 0.1, True), (300, 'gini', 0.2,True), \n",
    "    \n",
    "    (500, 'entropy', 'auto', False), (500, 'entropy', 'auto', False), \n",
    "    (500, 'entropy', 0.05, False), (500, 'entropy', 0.1, False), \n",
    "    (500, 'entropy', 0.2, False), \n",
    "    \n",
    "    (200, 'entropy', 'auto', False), (200, 'entropy', 0.05, False), \n",
    "    (200, 'entropy', 0.1, False), (200, 'entropy', 0.2, False), \n",
    "    \n",
    "    (300, 'entropy', 'auto', False), (300, 'entropy', 0.05, False), \n",
    "    (300, 'entropy', 0.1, False), (300, 'entropy', 0.2, False), \n",
    "    \n",
    "    (500, 'gini', 'auto', False), (500, 'gini', 0.05, False), \n",
    "    (500, 'gini', 0.1, False), (500, 'gini', 0.2, False), \n",
    "    \n",
    "    (200, 'gini', 'auto', False), (200, 'gini', 0.05, False), \n",
    "    (200, 'gini', 0.1, False), (200, 'gini', 0.2, False), \n",
    "    \n",
    "    (300, 'gini', 'auto', False), (300, 'gini', 0.05, False), \n",
    "    (300, 'gini', 0.1, False), (300, 'gini', 0.2,False), \n",
    "    \n",
    "]\n",
    "\n",
    "print(len(hyper_pram_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "\n",
    "try: \n",
    "    filename = 'randomforest_predictions_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    all_y_preds = pickle.load(open(filepath, 'rb+') )\n",
    "    \n",
    "    filename = 'randomforest_accs_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    all_eval_acc = pickle.load(open(filepath, 'rb+') )\n",
    "except: \n",
    "    all_y_preds = []\n",
    "    all_eval_acc = []\n",
    "\n",
    "\n",
    "\n",
    "for i, (n_est, criterion, max_features, oob_score) in enumerate(hyper_pram_list): \n",
    "    \n",
    "    if i < len(all_y_preds): \n",
    "        continue\n",
    "    \n",
    "    avg_score = 0\n",
    "\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=n_est, criterion=criterion, n_jobs=-1, \n",
    "                                               max_features=max_features, oob_score=oob_score)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_val, y_val)\n",
    "        avg_score += score \n",
    "\n",
    "    avg_score = avg_score/n_splits\n",
    "    \n",
    "    print( (n_est, criterion, max_features, oob_score), '\\t AVG:', avg_score,  '\\n')\n",
    "\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, criterion=criterion, n_jobs=-1, \n",
    "                                               max_features=max_features, oob_score=oob_score)\n",
    "    clf.fit(X, y)\n",
    "    y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "    all_y_preds.append(y_pred)\n",
    "    all_eval_acc.append(avg_score)\n",
    "    \n",
    "    \n",
    "    filename = 'randomforest_predictions_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    pickle.dump(all_y_preds, open(filepath, 'wb+') )\n",
    "\n",
    "\n",
    "    filename = 'randomforest_accs_prob.pk'\n",
    "    filepath = os.path.join('../data/coupons/all_outputs/', filename)\n",
    "    pickle.dump(all_eval_acc, open(filepath, 'wb+') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB, CategoricalNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score = 0\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_est, criterion=criterion, n_jobs=-1, \n",
    "                                           max_features=max_features, oob_score=oob_score)\n",
    "    \n",
    "#     clf = xgb.XGBClassifier(n_estimators=n_est, max_depth=max_depth, gamma=gamma)\n",
    "    \n",
    "#     clf = KNeighborsClassifier(n_neighbors=40, weights='distance', n_jobs=-1, p=1)\n",
    "    \n",
    "#     clf = QuadraticDiscriminantAnalysis(reg_param=0.2)\n",
    "    \n",
    "#     clf = AdaBoostClassifier(n_estimators=500, base_estimator=tree.DecisionTreeClassifier(max_depth=2))\n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_val, y_val)\n",
    "    avg_score += score \n",
    "\n",
    "avg_score = avg_score/n_splits\n",
    "\n",
    "print( '\\t AVG:', avg_score,  '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Models \n",
    "\n",
    "1. RandomForestClassifier: 0.756578351209148\n",
    "    1. clf =  RandomForestClassifier(n_estimators=500, criterion='entropy', n_jobs=-1, \n",
    "                                           max_features=0.5)\n",
    "                                    \n",
    "                                   \n",
    "2. XGBClassifier: 0.7647283152518649\n",
    "    1. clf = xgb.XGBClassifier(n_estimators=500)\n",
    "\n",
    "                                     \n",
    "3. Simple NeuralNetwork: 0.77287948010914\n",
    "\n",
    "\n",
    "\n",
    "4. AdaBoostClassifier: 0.7396912493661685\n",
    "    1. clf = AdaBoostClassifier(n_estimators=500, base_estimator=tree.DecisionTreeClassifier(max_depth=2))\n",
    "    \n",
    "\n",
    "\n",
    "*********************\n",
    "\n",
    "### Average Models \n",
    "\n",
    "5. KNeighborsClassifier: 0.7231924476209388\n",
    "    1. clf = KNeighborsClassifier(n_neighbors=40, weights='distance', n_jobs=-1, p=1)\n",
    "\n",
    "\n",
    "    \n",
    "6. QuadraticDiscriminantAnalysis: 0.7120009683333768  \n",
    "    QuadraticDiscriminantAnalysis(reg_param=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbour (with Information-Gain weighted)\n",
    "\n",
    "## Did not work as expected.\n",
    "\n",
    "    1. Requires Information gains stored in pickle. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = np.array(X_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "avg_score = 0\n",
    "\n",
    "'''\n",
    "Simple KNN\n",
    "'''\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=40, weights='distance', n_jobs=-1, p=1)\n",
    "\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_val, y_val)\n",
    "    avg_score += score \n",
    "    print(score)\n",
    "\n",
    "avg_score = avg_score/n_splits\n",
    "print('AVG:', avg_score)\n",
    "\n",
    "\n",
    "info_gain_results = pickle.load(open('../outputs/info_gains.pk', 'rb+') )\n",
    "info_gain_results_dict = {}\n",
    "\n",
    "for (k, v) in info_gain_results: \n",
    "    info_gain_results_dict[k] = v\n",
    "    \n",
    "\n",
    "\n",
    "X = np.array(X_df)\n",
    "y = np.array(y_df)\n",
    "\n",
    "weights = np.array([info_gain_results_dict[item] for item in list(X_df.columns) ]).reshape(1, -1)\n",
    "\n",
    "\n",
    "S = np.ones( (X.shape[0], 1))*weights\n",
    "\n",
    "X = np.multiply(X, S)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "KNN with weighted numbers\n",
    "'''\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "avg_score = 0\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=40, weights='distance', n_jobs=-1, p=1)\n",
    "\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_val, y_val)\n",
    "    avg_score += score \n",
    "    print(score)\n",
    "\n",
    "avg_score = avg_score/n_splits\n",
    "print('AVG:', avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
